{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc56d80",
   "metadata": {},
   "source": [
    "##### This notebook contains implementations of SVM, RF and LR using the doc2vec and sent2vec embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2205384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm import trange\n",
    "import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fa82f",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = r\"F:\\LJPE-Dataset\\LJPE Dataset2.0.csv\"\n",
    "data = pd.read_csv(path_to_dataset)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1261102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b9eea",
   "metadata": {},
   "source": [
    "## doc2vec (dimension = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845eefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_path = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\results\"\n",
    "if not os.path.exists(saved_path):\n",
    "    os.makedirs(saved_path)\n",
    "\n",
    "path_train = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\doc2vec\\train_LJPE_500_doc2vec.npy\"\n",
    "# path_train_labels = sys.argv[2]\n",
    "path_test = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\doc2vec\\test_LJPE_500_doc2vec.npy\"\n",
    "# path_test_labels = sys.argv[4]\n",
    "path_dev = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\doc2vec\\val_LJPE_500_doc2vec.npy\"\n",
    "# path_dev_labels = 'val_single_ILDC_500_DOC2VEC.npy'\n",
    "\n",
    "x_train=np.load(path_train)\n",
    "# y_train=np.load(path_train_labels)\n",
    "x_test=np.load(path_test)\n",
    "# y_test=np.load(path_test_labels)\n",
    "x_dev=np.load(path_dev)\n",
    "# y_dev=np.load(path_dev_labels)\n",
    "\n",
    "grouped = df.groupby('split')\n",
    "train_df = grouped.get_group('train')\n",
    "test_df = grouped.get_group('test')\n",
    "val_df = grouped.get_group('val')\n",
    "\n",
    "\n",
    "y_train = train_df['encoded_judgment']\n",
    "y_test = test_df['encoded_judgment']\n",
    "y_dev = val_df['encoded_judgment']\n",
    "\n",
    "\n",
    "#Utility function that calculates the metric scores given the predicted and true labels\n",
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP, FP, FN = [], [], []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    precision = [i for i in precision if not np.isnan(i)]\n",
    "    recall = [i for i in recall if not np.isnan(i)]\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "\n",
    "# RF utility function that creates the file RF_results.txt in which we vary the n_estimators parameter from 50 to 500\n",
    "# with increments of 50\n",
    "def RF_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"RF_results(doc500).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    print('Fitting RandomForestClassifier')\n",
    "    f.write(\"Varying the n_estimators from 50 to 1000\\n\\n\")\n",
    "    for n_est in tqdm.tqdm(range(50,1000,50)):\n",
    "        clf=RandomForestClassifier(n_estimators=n_est)\n",
    "        clf.fit(train_avg,train_labels)\n",
    "        d_preds = clf.predict(dev_avg)\n",
    "        Heading = \"For n_estimators: \" + str(n_est) + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") +str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-------Dev set-------\\n\"+ d_res + d_metrics)\n",
    "        \n",
    "        t_preds = clf.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \"  + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"-----Test set------\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "# LR utility function that creates the file RF_results.txt in which we vary the max_iters parameter from 50 to 500\n",
    "# with increments of 50\n",
    "def LR_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"LR_results(doc500).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the max_iters from 50 to 1000\\n\\n\")\n",
    "    print('Fitting LogisticRegression')\n",
    "    for it in tqdm.tqdm(range(50,1000,50)):\n",
    "        LR = LogisticRegression(C=1, max_iter =it)\n",
    "        LR.fit(train_avg, train_labels)\n",
    "        d_preds = LR.predict(dev_avg)\n",
    "        Heading = \"For max_iters: \" + str(it) + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-----Dev set-----\\n\"+ d_res + d_metrics)\n",
    "        \n",
    "        t_preds = LR.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" +  str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"------Test set------\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "# SVM utility function that gives results by creating file \"SVM_avgi_results.txt\" in the working\n",
    "# directory\n",
    "# Remember that in the hyperparams we are only varying the kernels here from linear to poly to rbf\n",
    "def SVM_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"SVM_results(doc500).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the kernels: \\n\\n\")\n",
    "    kers = [\"linear\", \"poly\", \"rbf\"]\n",
    "    for k in kers:\n",
    "        print(\"Running for {0}\".format(k))\n",
    "        SVM = svm.SVC(C=1, kernel=k)\n",
    "        SVM.fit(train_avg, train_labels)\n",
    "        d_preds = SVM.predict(dev_avg)\n",
    "        Heading = \"For kernel: \" + k + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-------Dev set------\\n\"+ d_res + d_metrics)\n",
    "\n",
    "        t_preds = SVM.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" |\" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"------Test set------\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "# os.chdir(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and get the results for each model\n",
    "\n",
    "LR_scores(x_train, x_dev, x_test, y_train, y_dev, y_test, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_scores(x_train, x_dev, x_test, y_train, y_dev, y_test, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_scores(x_train, x_dev, x_test, y_train, y_dev, y_test, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb9880",
   "metadata": {},
   "source": [
    "## doc2vec (dimension = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde55c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_path = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\results\"\n",
    "if not os.path.exists(saved_path):\n",
    "  os.makedirs(saved_path)\n",
    "\n",
    "path_train = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\doc2vec\\train_LJPE_1000_doc2vec.npy\"\n",
    "# path_train_labels = sys.argv[2]\n",
    "path_test = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\doc2vec\\test_LJPE_1000_doc2vec.npy\"\n",
    "# path_test_labels = sys.argv[4]\n",
    "path_dev = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\doc2vec\\val_LJPE_1000_doc2vec.npy\"\n",
    "# path_dev_labels = 'val_single_ILDC_500_DOC2VEC.npy'\n",
    "\n",
    "x_train=np.load(path_train)\n",
    "# y_train=np.load(path_train_labels)\n",
    "x_test=np.load(path_test)\n",
    "# y_test=np.load(path_test_labels)\n",
    "x_dev=np.load(path_dev)\n",
    "# y_dev=np.load(path_dev_labels)\n",
    "\n",
    "grouped = df.groupby('split')\n",
    "train_df = grouped.get_group('train')\n",
    "test_df = grouped.get_group('test')\n",
    "val_df = grouped.get_group('val')\n",
    "\n",
    "\n",
    "y_train = train_df['encoded_judgment']\n",
    "y_test = test_df['encoded_judgment']\n",
    "y_dev = val_df['encoded_judgment']\n",
    "\n",
    "\n",
    "#Utility function that calculates the metric scores given the predicted and true labels\n",
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP, FP, FN = [], [], []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "        \n",
    "    precision = [i for i in precision if not np.isnan(i)]\n",
    "    recall = [i for i in recall if not np.isnan(i)]\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "# RF utility function that creates the file RF_results.txt in which we vary the n_estimators parameter from 50 to 500\n",
    "# with increments of 50\n",
    "def RF_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"RF_results(doc1000).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the n_estimators from 50 to 1000\\n\\n\")\n",
    "    print('Fitting RandomForestClassifier')\n",
    "    for n_est in tqdm.tqdm(range(50,500,50)):\n",
    "        clf=RandomForestClassifier(n_estimators=n_est)\n",
    "        clf.fit(train_avg,train_labels)\n",
    "        d_preds = clf.predict(dev_avg)\n",
    "        Heading = \"For n_estimators: \" + str(n_est) + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros: \" + str(\"macro_precision \") +str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-----Dev set----\\n\"+ d_res + d_metrics)\n",
    "        \n",
    "        t_preds = clf.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros: \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"-----Test set-----\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "# LR utility function that creates the file RF_results.txt in which we vary the max_iters parameter from 50 to 500\n",
    "# with increments of 50\n",
    "def LR_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"LR_results(doc1000).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    print('Fitting LogisticRegression')\n",
    "    f.write(\"Varying the max_iters from 50 to 1000\\n\\n\")\n",
    "    for it in tqdm.tqdm(range(50,1000,50)):\n",
    "        LR = LogisticRegression(C=1, max_iter =it)\n",
    "        LR.fit(train_avg, train_labels)\n",
    "        d_preds = LR.predict(dev_avg)\n",
    "        Heading = \"For max_iters: \" + str(it) + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-----Dev set-----\\n\"+ d_res + d_metrics)\n",
    "        \n",
    "        t_preds = LR.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"-----Test set-----\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "# SVM utility function that gives results by creating file \"SVM_avgi_results.txt\" in the working\n",
    "# directory\n",
    "# Remember that in the hyperparams we are only varying the kernels here from linear to poly to rbf\n",
    "def SVM_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"SVM_results(doc1000).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the kernels: \\n\\n\")\n",
    "    kers = [\"linear\", \"poly\", \"rbf\"]\n",
    "    for k in kers:\n",
    "        print(\"Running for {0}\".format(k))\n",
    "        SVM = svm.SVC(C=1, kernel=k)\n",
    "        SVM.fit(train_avg, train_labels)\n",
    "        d_preds = SVM.predict(dev_avg)\n",
    "        Heading = \"For kernel: \" + k + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-----Dev set-----\\n\"+ d_res + d_metrics)\n",
    "\n",
    "        t_preds = SVM.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") +  str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") +  str(macro_f1) + \"\\n\"\n",
    "        f.write(\"-----Test set-----\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and get the results for each model\n",
    "\n",
    "LR_scores(x_train, x_dev, x_test, y_train, y_dev, y_test, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73709774",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_scores(x_train, x_dev, x_test, y_train, y_dev, y_test, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_scores(x_train, x_dev, x_test, y_train, y_dev, y_test, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997202e8",
   "metadata": {},
   "source": [
    "## sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a96a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File saving folder name. \n",
    "saved_path = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\results\"\n",
    "if not os.path.exists(saved_path):\n",
    "  os.makedirs(saved_path)\n",
    "\n",
    "\n",
    "path_x = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\sent2vec\\train_LJPE_200_sent2vec.npy\"\n",
    "path_y = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\sent2vec\\test_LJPE_200_sent2vec.npy\"\n",
    "path_z = r\"F:\\[LJPE] Current Models\\Embeddings (Classical Models)\\LJPE Embeddings\\sent2vec\\val_LJPE_200_sent2vec.npy\"\n",
    "\n",
    "\n",
    "# Here we are averaging out the embeddings of all the sentence embeddings of a document.\n",
    "# We have also tried out max and min pooling as well but they were giving worse results.\n",
    "# Modify the code here accordingly if you want to try out those methods as well.\n",
    "\n",
    "def average_out_embeddings(npy_file_path):\n",
    "    x=np.load(npy_file_path,allow_pickle=True)\n",
    "    size = x.shape[0]\n",
    "    x_split_data=x\n",
    "    # y_split=x[:,1]\n",
    "    # y_split=y_split.astype('int')\n",
    "    x_split=np.zeros([size,200])\n",
    "    for i in range(0,size):\n",
    "        b=np.zeros([len(list(x_split_data[i])),200])\n",
    "        for j in range(0,len(x_split_data[i])):\n",
    "            b[j,:]=x_split_data[i][j]\n",
    "        b=np.sum(b,axis=0)\n",
    "        x_split[i,:]=b/len(x_split_data[i])\n",
    "    return x_split\n",
    "\n",
    "#prepare data for each set by averaging out sentence embeddings\n",
    "x_train = average_out_embeddings(path_x)\n",
    "x_test = average_out_embeddings(path_y)\n",
    "x_dev = average_out_embeddings(path_z)\n",
    "\n",
    "grouped = df.groupby('split')\n",
    "train_df = grouped.get_group('train')\n",
    "test_df = grouped.get_group('test')\n",
    "val_df = grouped.get_group('val')\n",
    "\n",
    "\n",
    "y_train1 = train_df['encoded_judgment']\n",
    "y_test1 = test_df['encoded_judgment']\n",
    "y_dev1 = val_df['encoded_judgment']\n",
    "\n",
    "\n",
    "#Utility function that calculates the metric scores given the predicted and true labels\n",
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP, FP, FN = [], [], []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    precision = [i for i in precision if not np.isnan(i)]\n",
    "    recall = [i for i in recall if not np.isnan(i)]\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    # print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "\n",
    "# SVM utility function that gives results by creating file \"SVM_avgi_results.txt\" in the working\n",
    "# directory\n",
    "# Remember that in the hyperparams we are only varying the kernels here from linear to poly to rbf\n",
    "def SVM_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"SVM_avgi_results(sent200).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the kernels: \\n\\n\")\n",
    "    kers = [\"linear\", \"poly\", \"rbf\"]\n",
    "    for k in kers:\n",
    "        print(\"Running for {0}\".format(k))\n",
    "        SVM = svm.SVC(C=1, kernel=k)\n",
    "        SVM.fit(train_avg, train_labels)\n",
    "        d_preds = SVM.predict(dev_avg)\n",
    "        Heading = \"For kernel: \" + k + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"------Dev set-----\\n\"+ d_res + d_metrics)\n",
    "\n",
    "        t_preds = SVM.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" +  str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"-----Test set-----\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "# RF utility function that creates the file RF_results.txt in which we vary the n_estimators parameter from 50 to 1000\n",
    "# with increments of 50\n",
    "def RF_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"RF_results(sent200).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the n_estimators from 50 to 1000\\n\\n\")\n",
    "    print('Fitting RandomForestClassifier')\n",
    "    for n_est in tqdm.tqdm(range(50,1000,50)):\n",
    "        clf=RandomForestClassifier(n_estimators=n_est)\n",
    "        clf.fit(train_avg,train_labels)\n",
    "        d_preds = clf.predict(dev_avg)\n",
    "        Heading = \"For n_estimators: \" + str(n_est) + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros ->\" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"----Dev set----\\n\"+ d_res + d_metrics)\n",
    "        \n",
    "        t_preds = clf.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) + \" | \" + str(\"macro_f1 \") +  str(macro_f1) + \"\\n\"\n",
    "        f.write(\"----Test set----\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "# LR utility function that creates the file RF_results.txt in which we vary the max_iters parameter from 50 to 1000\n",
    "# with increments of 50\n",
    "def LR_scores(train_avg, dev_avg, test_avg, train_labels, dev_labels, test_labels,saved_path):\n",
    "    file_path = os.path.join(saved_path,\"LR_results(sent200).txt\")\n",
    "    f = open(file_path, \"w+\")\n",
    "    f.write(\"Varying the max_iters from 50 to 1000\\n\\n\")\n",
    "    print('Fitting LogisticRegression')\n",
    "    for it in tqdm.tqdm(range(50,1000,50)):\n",
    "        LR = LogisticRegression(C=1, max_iter =it)\n",
    "        LR.fit(train_avg, train_labels)\n",
    "        d_preds = LR.predict(dev_avg)\n",
    "        Heading = \"For max_iters: \" + str(it) + \"\\n\"\n",
    "        d_res = \"Accuracy -> \" + str(accuracy_score(d_preds, dev_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(d_preds, dev_labels)\n",
    "        d_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) +  \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(Heading + \"-----Dev set-----\\n\"+ d_res + d_metrics)\n",
    "        \n",
    "        t_preds = LR.predict(test_avg)\n",
    "        t_res = \"Accuracy -> \" + str(accuracy_score(t_preds, test_labels)*100) + \"\\n\"\n",
    "        macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(t_preds, test_labels)\n",
    "        t_metrics = \"Micros -> \" + str(\"micro_precision \") + str(micro_precision) + \" | \" + str(\"micro_recall \") + str(micro_recall) + \" | \" + str(\"micro_f1 \") + str(micro_f1) + \" \\nMacros -> \" + str(\"macro_precision \") + str(macro_precision) + \" | \" + str(\"macro_recall \") + str(macro_recall) +  \" | \" + str(\"macro_f1 \") + str(macro_f1) + \"\\n\"\n",
    "        f.write(\"----Test set----\\n\"+ t_res + t_metrics + \"\\n\\n\")\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68410ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and get the results for each model\n",
    "LR_scores(x_train, x_dev, x_test, y_train1, y_dev1, y_test1, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_scores(x_train, x_dev, x_test, y_train1, y_dev1, y_test1, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd637bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_scores(x_train, x_dev, x_test, y_train1, y_dev1, y_test1, saved_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
